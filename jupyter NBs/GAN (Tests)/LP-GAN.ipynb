{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0ea33c",
   "metadata": {},
   "source": [
    "# LP-GAN Training\n",
    "\n",
    "This notebook demonstrates training an LP-GAN (Lp Gradient Penalty GAN) on various datasets like Shapes3D, Cars3D, and DSprites.\n",
    "\n",
    "The LP-GAN uses Wasserstein loss with Lp gradient penalty to enforce the Lipschitz constraint on the discriminator, leading to more stable training compared to vanilla GANs. The gradient penalty typically uses L2 norm (p=2) but can be generalized to other Lp norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Add the project root to the path to import our modules\n",
    "sys.path.append('jupyter NBs/GAN (Tests)')\n",
    "\n",
    "# Import our custom modules\n",
    "from datasets.shapes3d import Shapes3D\n",
    "from gan.trainer import GANTrainer\n",
    "from gan.architecture import Generator, Discriminator\n",
    "from gan.loss import get_loss\n",
    "from gan.utils import show_dataset_samples, free_memory, set_random_seeds\n",
    "\n",
    "# Set up device and random seeds for reproducibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory cleanup utility - now using imported function from utils\n",
    "# Example: call after training (you can also call manually later)\n",
    "free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility - now using imported function from utils\n",
    "seed_number = 42\n",
    "set_random_seeds(seed_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b039f0a",
   "metadata": {},
   "source": [
    "# Shapes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configuration\n",
    "print(\"Setting up Shapes3D dataset...\")\n",
    "\n",
    "# For this experiment, we'll allow all factors to vary\n",
    "# This gives us the full diversity of the dataset\n",
    "selected_factors = ['floorCol', 'wallCol', 'objCol', 'objSize', 'objType', 'objAzimuth']\n",
    "not_selected_factors_index_value = {}  # Empty since we're selecting all factors\n",
    "\n",
    "# root = '/data/amin/shapes3d/'  # Adjust this path as needed\n",
    "root = 'data/shapes3d/'  # Local path for testing\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = Shapes3D(\n",
    "    selected_factors=selected_factors,\n",
    "    not_selected_factors_index_value=not_selected_factors_index_value,\n",
    "    root=root,\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded with {len(dataset)} images\")\n",
    "print(f\"Image size: {dataset.img_size}\")\n",
    "print(f\"Selected factors: {dataset.selected_factors}\")\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=7 if device.type == 'cuda' else 0,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created with batch size: {batch_size}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0416d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample data\n",
    "print(\"Sample images from the Shapes3D dataset:\")\n",
    "show_dataset_samples(dataset, n_samples=16, title='Shapes3D Dataset Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b60fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LP-GAN Configuration\n",
    "print(\"Setting up LP-GAN trainer...\")\n",
    "\n",
    "# Model hyperparameters\n",
    "latent_dim = 10\n",
    "img_size = (3, 64, 64)  # C, H, W\n",
    "learning_rate_g = 1e-4\n",
    "learning_rate_d = 1e-4\n",
    "beta1 = 0.5  # LP-GAN typically uses beta1=0.5 for Adam\n",
    "beta2 = 0.999\n",
    "\n",
    "# LP-GAN specific parameters\n",
    "lambda_gp = 10.0  # Gradient penalty coefficient\n",
    "p_norm = 2  # Order of Lp norm (L2 penalty)\n",
    "\n",
    "# Create models explicitly (no more defaults inside GANTrainer)\n",
    "generator = Generator(latent_dim=latent_dim, img_size=img_size, use_batchnorm=True, negative_slope=0.2, output_activation='sigmoid')\n",
    "# LP-GAN doesn't require spectral normalization, uses gradient penalty instead\n",
    "discriminator = Discriminator(img_size=img_size, use_spectral_norm=False)\n",
    "\n",
    "# Move models to device\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "# Create optimizers explicitly\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate_g, betas=(beta1, beta2))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate_d, betas=(beta1, beta2))\n",
    "\n",
    "# Create LP-GAN trainer (now requires models and optimizers)\n",
    "trainer = GANTrainer(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    g_optimizer=g_optimizer,\n",
    "    d_optimizer=d_optimizer,\n",
    "    loss_type='lpgan',  # Lp gradient penalty loss\n",
    "    loss_kwargs={'lambda_gp': lambda_gp, 'p': p_norm},\n",
    "    device=device,\n",
    "    n_critic=1  # Update generator every discriminator update\n",
    ")\n",
    "\n",
    "print(f\"LP-GAN trainer created\")\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in trainer.generator.parameters()):,}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in trainer.discriminator.parameters()):,}\")\n",
    "print(f\"Loss type: {trainer.loss_type}\")\n",
    "print(f\"Gradient penalty coefficient (λ_gp): {lambda_gp}\")\n",
    "print(f\"Lp norm order (p): {p_norm}\")\n",
    "print(f\"Device: {trainer.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a333e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "epochs = 1  # Adjust based on your computational resources\n",
    "\n",
    "# Option 1: Train by epochs (existing functionality)\n",
    "trainer.train(dataloader, epochs=epochs)\n",
    "\n",
    "# Option 2: Train by iterations (new functionality)\n",
    "# You can also train by specifying total iterations instead of epochs\n",
    "# This gives you precise control over training duration regardless of dataset size\n",
    "# Example:\n",
    "# total_iterations = epochs * len(dataloader)  # equivalent to the epochs above\n",
    "# trainer.train(dataloader, total_iterations=total_iterations)\n",
    "# \n",
    "# Or train for a specific number of iterations:\n",
    "# trainer.train(dataloader, total_iterations=1000)  # train for exactly 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac71fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training Progress\n",
    "print(\"Plotting training losses...\")\n",
    "trainer.plot_losses()\n",
    "\n",
    "# Show training history summary\n",
    "if trainer.history['d_loss'] and trainer.history['g_loss']:\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"Final Discriminator Loss: {trainer.history['d_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Generator Loss: {trainer.history['g_loss'][-1]:.4f}\")\n",
    "    print(f\"Average Discriminator Loss: {np.mean(trainer.history['d_loss']):.4f}\")\n",
    "    print(f\"Average Generator Loss: {np.mean(trainer.history['g_loss']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and Display Sample Images\n",
    "print(\"Generating sample images from trained LP-GAN...\")\n",
    "trainer.plot_samples(n_samples=25, n_cols=5, figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356cfbe1",
   "metadata": {},
   "source": [
    "# DSprites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447212e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configuration for DSprites\n",
    "print(\"Setting up DSprites dataset...\")\n",
    "\n",
    "# Import DSprites dataset\n",
    "from datasets.dsprites import DSprites\n",
    "\n",
    "# For dsprites, we'll select all factors except color (which has only one value)\n",
    "# DSprites factors: ('color', 'shape', 'scale', 'orientation', 'posX', 'posY')\n",
    "# selected_factors = ['shape', 'scale', 'orientation', 'posX', 'posY']\n",
    "# not_selected_factors_index_value = {'color': 0}  # Fix color to its only value\n",
    "\n",
    "selected_factors = ['orientation', 'posX', 'posY']  # Example selection for dsprites\n",
    "not_selected_factors_index_value = {'color': 0, 'shape':0, 'scale': 5}  \n",
    "\n",
    "\n",
    "root = 'data/dsprites/'  # Local path for dsprites dataset\n",
    "\n",
    "# Create dataset instance\n",
    "dataset_dsprites = DSprites(\n",
    "    selected_factors=selected_factors,\n",
    "    not_selected_factors_index_value=not_selected_factors_index_value,\n",
    "    root=root,\n",
    "    drop_color_factor=True  # Drop color factor since it has only one value\n",
    ")\n",
    "\n",
    "print(f\"DSprites dataset loaded with {len(dataset_dsprites)} images\")\n",
    "print(f\"Image size: {dataset_dsprites.img_size}\")\n",
    "print(f\"Selected factors: {dataset_dsprites.selected_factors}\")\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size_dsprites = 64\n",
    "dataloader_dsprites = DataLoader(\n",
    "    dataset_dsprites, \n",
    "    batch_size=batch_size_dsprites, \n",
    "    shuffle=True, \n",
    "    num_workers=7 if device.type == 'cuda' else 0,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created with batch size: {batch_size_dsprites}\")\n",
    "print(f\"Number of batches: {len(dataloader_dsprites)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample data from DSprites\n",
    "print(\"Sample images from the DSprites dataset:\")\n",
    "show_dataset_samples(dataset_dsprites, n_samples=16, title='DSprites Dataset Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LP-GAN Configuration for DSprites\n",
    "print(\"Setting up LP-GAN trainer for DSprites...\")\n",
    "\n",
    "# Model hyperparameters for DSprites (grayscale images)\n",
    "latent_dim_dsprites = 10\n",
    "img_size_dsprites = (1, 64, 64)  # Grayscale images (1 channel)\n",
    "learning_rate_g_dsprites = 1e-4\n",
    "learning_rate_d_dsprites = 1e-4\n",
    "beta1_dsprites = 0.5  # LP-GAN typically uses beta1=0.5\n",
    "beta2_dsprites = 0.999\n",
    "\n",
    "# LP-GAN specific parameters\n",
    "lambda_gp_dsprites = 10.0  # Gradient penalty coefficient\n",
    "p_norm_dsprites = 2  # Order of Lp norm (L2 penalty)\n",
    "\n",
    "# Create models for DSprites (1-channel input)\n",
    "generator_dsprites = Generator(latent_dim=latent_dim_dsprites, img_size=img_size_dsprites, output_activation='sigmoid')\n",
    "# LP-GAN doesn't require spectral normalization, uses gradient penalty instead\n",
    "discriminator_dsprites = Discriminator(img_size=img_size_dsprites, use_spectral_norm=False)\n",
    "\n",
    "# Move models to device\n",
    "generator_dsprites = generator_dsprites.to(device)\n",
    "discriminator_dsprites = discriminator_dsprites.to(device)\n",
    "\n",
    "# Create optimizers\n",
    "g_optimizer_dsprites = optim.Adam(generator_dsprites.parameters(), lr=learning_rate_g_dsprites, betas=(beta1_dsprites, beta2_dsprites))\n",
    "d_optimizer_dsprites = optim.Adam(discriminator_dsprites.parameters(), lr=learning_rate_d_dsprites, betas=(beta1_dsprites, beta2_dsprites))\n",
    "\n",
    "# Create LP-GAN trainer for DSprites\n",
    "trainer_dsprites = GANTrainer(\n",
    "    generator=generator_dsprites,\n",
    "    discriminator=discriminator_dsprites,\n",
    "    g_optimizer=g_optimizer_dsprites,\n",
    "    d_optimizer=d_optimizer_dsprites,\n",
    "    loss_type='lpgan',  # Lp gradient penalty loss\n",
    "    loss_kwargs={'lambda_gp': lambda_gp_dsprites, 'p': p_norm_dsprites},\n",
    "    device=device,\n",
    "    n_critic=1  # Update generator every discriminator update\n",
    ")\n",
    "\n",
    "print(f\"LP-GAN trainer created for DSprites\")\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in trainer_dsprites.generator.parameters()):,}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in trainer_dsprites.discriminator.parameters()):,}\")\n",
    "print(f\"Loss type: {trainer_dsprites.loss_type}\")\n",
    "print(f\"Gradient penalty coefficient (λ_gp): {lambda_gp_dsprites}\")\n",
    "print(f\"Lp norm order (p): {p_norm_dsprites}\")\n",
    "print(f\"Device: {trainer_dsprites.device}\")\n",
    "print(f\"Image size: {img_size_dsprites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65bc4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration for DSprites\n",
    "epochs_dsprites = 20  # Adjust based on your computational resources\n",
    "print(f\"Starting LP-GAN training on DSprites for {epochs_dsprites} epochs...\")\n",
    "\n",
    "# Option 1: Train by epochs (existing functionality)\n",
    "trainer_dsprites.train(dataloader_dsprites, epochs=epochs_dsprites)\n",
    "\n",
    "# Option 2: Train by iterations (new functionality)\n",
    "# Alternative approach - train by specifying total iterations\n",
    "# total_iterations = epochs_dsprites * len(dataloader_dsprites)\n",
    "# trainer_dsprites.train(dataloader_dsprites, total_iterations=total_iterations)\n",
    "#\n",
    "# Or train for a fixed number of iterations regardless of dataset size:\n",
    "# trainer_dsprites.train(dataloader_dsprites, total_iterations=5000)\n",
    "#\n",
    "# You can also specify custom log intervals for iteration-based training:\n",
    "# trainer_dsprites.train(dataloader_dsprites, total_iterations=5000, log_interval=500)\n",
    "\n",
    "print(\"DSprites training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222766c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DSprites Training Progress\n",
    "print(\"Plotting DSprites training losses...\")\n",
    "trainer_dsprites.plot_losses()\n",
    "\n",
    "# Show training history summary for DSprites\n",
    "if trainer_dsprites.history['d_loss'] and trainer_dsprites.history['g_loss']:\n",
    "    print(f\"\\nDSprites Training Summary:\")\n",
    "    print(f\"Final Discriminator Loss: {trainer_dsprites.history['d_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Generator Loss: {trainer_dsprites.history['g_loss'][-1]:.4f}\")\n",
    "    print(f\"Average Discriminator Loss: {np.mean(trainer_dsprites.history['d_loss']):.4f}\")\n",
    "    print(f\"Average Generator Loss: {np.mean(trainer_dsprites.history['g_loss']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and Display Sample Images from DSprites LP-GAN\n",
    "print(\"Generating sample images from trained LP-GAN on DSprites...\")\n",
    "trainer_dsprites.plot_samples(n_samples=25, n_cols=5, figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb0cdf5",
   "metadata": {},
   "source": [
    "# Cars3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configuration for Cars3D\n",
    "print(\"Setting up Cars3D dataset...\")\n",
    "\n",
    "# Import Cars3D dataset\n",
    "from datasets.cars3d import Cars3D\n",
    "\n",
    "# For Cars3D, we'll select all factors to show the full diversity\n",
    "# Cars3D factors: ('elevation', 'azimuth', 'object_type')\n",
    "selected_factors = ['elevation', 'azimuth', 'object_type']\n",
    "not_selected_factors_index_value = {}  # Empty since we're selecting all factors\n",
    "\n",
    "root = 'data/cars3d/'  # Local path for cars3d dataset\n",
    "\n",
    "# Create dataset instance\n",
    "dataset_cars3d = Cars3D(\n",
    "    selected_factors=selected_factors,\n",
    "    not_selected_factors_index_value=not_selected_factors_index_value,\n",
    "    root=root,\n",
    ")\n",
    "\n",
    "print(f\"Cars3D dataset loaded with {len(dataset_cars3d)} images\")\n",
    "print(f\"Image size: {dataset_cars3d.img_size}\")\n",
    "print(f\"Selected factors: {dataset_cars3d.selected_factors}\")\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size_cars3d = 64\n",
    "dataloader_cars3d = DataLoader(\n",
    "    dataset_cars3d, \n",
    "    batch_size=batch_size_cars3d, \n",
    "    shuffle=True, \n",
    "    num_workers=7 if device.type == 'cuda' else 0,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created with batch size: {batch_size_cars3d}\")\n",
    "print(f\"Number of batches: {len(dataloader_cars3d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b86786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample data from Cars3D\n",
    "print(\"Sample images from the Cars3D dataset:\")\n",
    "show_dataset_samples(dataset_cars3d, n_samples=16, title='Cars3D Dataset Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92725395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LP-GAN Configuration for Cars3D\n",
    "print(\"Setting up LP-GAN trainer for Cars3D...\")\n",
    "\n",
    "# Model hyperparameters for Cars3D (RGB images, same as Shapes3D)\n",
    "latent_dim_cars3d = 10\n",
    "img_size_cars3d = (3, 64, 64)  # RGB images (3 channels)\n",
    "learning_rate_g_cars3d = 1e-4\n",
    "learning_rate_d_cars3d = 1e-4\n",
    "beta1_cars3d = 0.5  # LP-GAN typically uses beta1=0.5\n",
    "beta2_cars3d = 0.999\n",
    "\n",
    "# LP-GAN specific parameters\n",
    "lambda_gp_cars3d = 10.0  # Gradient penalty coefficient\n",
    "p_norm_cars3d = 2  # Order of Lp norm (L2 penalty)\n",
    "\n",
    "# Create models for Cars3D (3-channel RGB input, same as Shapes3D)\n",
    "generator_cars3d = Generator(latent_dim=latent_dim_cars3d, img_size=img_size_cars3d, use_batchnorm=True, negative_slope=0.2, output_activation='sigmoid')\n",
    "# LP-GAN doesn't require spectral normalization, uses gradient penalty instead\n",
    "discriminator_cars3d = Discriminator(img_size=img_size_cars3d, use_spectral_norm=False)\n",
    "\n",
    "# Move models to device\n",
    "generator_cars3d = generator_cars3d.to(device)\n",
    "discriminator_cars3d = discriminator_cars3d.to(device)\n",
    "\n",
    "# Create optimizers\n",
    "g_optimizer_cars3d = optim.Adam(generator_cars3d.parameters(), lr=learning_rate_g_cars3d, betas=(beta1_cars3d, beta2_cars3d))\n",
    "d_optimizer_cars3d = optim.Adam(discriminator_cars3d.parameters(), lr=learning_rate_d_cars3d, betas=(beta1_cars3d, beta2_cars3d))\n",
    "\n",
    "# Create LP-GAN trainer for Cars3D\n",
    "trainer_cars3d = GANTrainer(\n",
    "    generator=generator_cars3d,\n",
    "    discriminator=discriminator_cars3d,\n",
    "    g_optimizer=g_optimizer_cars3d,\n",
    "    d_optimizer=d_optimizer_cars3d,\n",
    "    loss_type='lpgan',  # Lp gradient penalty loss\n",
    "    loss_kwargs={'lambda_gp': lambda_gp_cars3d, 'p': p_norm_cars3d},\n",
    "    device=device,\n",
    "    n_critic=1  # Update generator every discriminator update\n",
    ")\n",
    "\n",
    "print(f\"LP-GAN trainer created for Cars3D\")\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in trainer_cars3d.generator.parameters()):,}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in trainer_cars3d.discriminator.parameters()):,}\")\n",
    "print(f\"Loss type: {trainer_cars3d.loss_type}\")\n",
    "print(f\"Gradient penalty coefficient (λ_gp): {lambda_gp_cars3d}\")\n",
    "print(f\"Lp norm order (p): {p_norm_cars3d}\")\n",
    "print(f\"Device: {trainer_cars3d.device}\")\n",
    "print(f\"Image size: {img_size_cars3d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeab32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration for Cars3D\n",
    "epochs_cars3d = 100  # Adjust based on your computational resources\n",
    "print(f\"Starting LP-GAN training on Cars3D for {epochs_cars3d} epochs...\")\n",
    "\n",
    "# Option 1: Train by epochs (existing functionality)\n",
    "trainer_cars3d.train(dataloader_cars3d, epochs=epochs_cars3d)\n",
    "\n",
    "# Option 2: Train by iterations (new functionality)\n",
    "# Alternative approach - train by specifying total iterations\n",
    "# total_iterations = epochs_cars3d * len(dataloader_cars3d)\n",
    "# trainer_cars3d.train(dataloader_cars3d, total_iterations=total_iterations)\n",
    "#\n",
    "# Or train for a fixed number of iterations regardless of dataset size:\n",
    "# trainer_cars3d.train(dataloader_cars3d, total_iterations=5000)\n",
    "#\n",
    "# You can also specify custom log intervals for iteration-based training:\n",
    "# trainer_cars3d.train(dataloader_cars3d, total_iterations=5000, log_interval=500)\n",
    "\n",
    "print(\"Cars3D training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Cars3D Training Progress\n",
    "print(\"Plotting Cars3D training losses...\")\n",
    "trainer_cars3d.plot_losses()\n",
    "\n",
    "# Show training history summary for Cars3D\n",
    "if trainer_cars3d.history['d_loss'] and trainer_cars3d.history['g_loss']:\n",
    "    print(f\"\\nCars3D Training Summary:\")\n",
    "    print(f\"Final Discriminator Loss: {trainer_cars3d.history['d_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Generator Loss: {trainer_cars3d.history['g_loss'][-1]:.4f}\")\n",
    "    print(f\"Average Discriminator Loss: {np.mean(trainer_cars3d.history['d_loss']):.4f}\")\n",
    "    print(f\"Average Generator Loss: {np.mean(trainer_cars3d.history['g_loss']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and Display Sample Images from Cars3D LP-GAN\n",
    "print(\"Generating sample images from trained LP-GAN on Cars3D...\")\n",
    "trainer_cars3d.plot_samples(n_samples=25, n_cols=5, figsize=(12, 12))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
