{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8d8f20",
   "metadata": {},
   "source": [
    "# S-N-Annealed-VAE Model Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root by looking for .git or requirements.txt\n",
    "current = Path.cwd()\n",
    "while not any((current / marker).exists() for marker in ['.git', 'requirements.txt']):\n",
    "    if current.parent == current:\n",
    "        raise FileNotFoundError(\"Project root not found\")\n",
    "    current = current.parent\n",
    "\n",
    "sys.path.append(str(current))\n",
    "print(f\"Added project root: {current}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c626977",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#### deterministic run ####\n",
    "determinism_kwargs = {\n",
    "    'seed':0,\n",
    "    'use_cuda_det': True,\n",
    "    'enforce_det':False,\n",
    "    'cublas_workspace_config': None,\n",
    "}\n",
    "\n",
    "##### Model parameters #####\n",
    "model_name = 's_n_vae_locatello'  # S-N-VAE model with Locatello architecture\n",
    "model_decoder_output_dist = 'bernoulli'  # Output distribution of the decoder\n",
    "\n",
    "# Define latent factor topologies: mix of Normal (R1) and Power Spherical (S1)\n",
    "latent_factor_topologies = ['S1', 'R1', 'R1']  # 3 factors total\n",
    "\n",
    "use_torch_compile = False  # Use torch.compile for model compilation\n",
    "\n",
    "#### Training parameters ####\n",
    "train_step_unit = 'epoch'  # Unit for training steps ('epoch' or 'iteration')\n",
    "num_train_steps = 300\n",
    "\n",
    "# train_step_unit = 'iteration'  # Unit for training steps ('epoch' or 'iteration')\n",
    "# num_train_steps = int(9e3)  # Number of training steps \n",
    "\n",
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "\n",
    "#### losses ####\n",
    "loss_name = 'anneal_s_n_vae'      # S-N-VAE Annealed loss\n",
    "loss_kwargs_dsprites = {\n",
    "    'C_init': 0.0,              # Starting annealed capacity\n",
    "    'C_fin': 5.0,               # Final annealed capacity\n",
    "    'gamma': 100.0,             # Weight of the KL divergence term\n",
    "    'anneal_steps': 50000,      # Number of steps for annealing (~2/3 of total training)\n",
    "    'latent_factor_topologies': latent_factor_topologies,\n",
    "    'rec_dist': 'bernoulli',    # Reconstruction distribution\n",
    "    'log_kl_components': True,\n",
    "#     'schedulers_kwargs':[\n",
    "#     {\n",
    "#         'name': 'linear',\n",
    "#         'kwargs': {\n",
    "#             'param_name': 'gamma',\n",
    "#             'initial_value': 50,\n",
    "#             'final_value': 200,\n",
    "#             'total_steps': 100000\n",
    "#         }\n",
    "#     }\n",
    "# ]\n",
    "}\n",
    "\n",
    "#### device parameters ####\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "### Checkpoint parameters ###\n",
    "return_chkpt = False\n",
    "chkpt_every_n_steps = 2400\n",
    "\n",
    "# chkpt_save_path = 'checkpoints/tests/test-epoch-1.pt'\n",
    "chkpt_save_path = None\n",
    "\n",
    "chkpt_save_dir = None\n",
    "# chkpt_save_dir = 'checkpoints/tests_s_n_vae'\n",
    "\n",
    "chkpt_save_master_dir = None\n",
    "chkpt_viz = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa456e",
   "metadata": {},
   "source": [
    "# Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcdfd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "if determinism_kwargs is not None:\n",
    "    # MUST Be set before importing any other modules\n",
    "    # to ensure reproducibility across all libraries\n",
    "    from utils.reproducibility import set_deterministic_run, get_deterministic_dataloader\n",
    "    set_deterministic_run(**determinism_kwargs)\n",
    "    print(f\"Set deterministic run with kwargs: {determinism_kwargs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185a7c83",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils.visualize\n",
    "from trainers import UnsupervisedTrainer\n",
    "import losses\n",
    "import vae_models\n",
    "from datasets import get_dataset\n",
    "from utils.io import find_optimal_num_workers\n",
    "from metrics.utils import MetricAggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bda9e5",
   "metadata": {},
   "source": [
    "## Setup Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867815d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_components(dataset, loss_kwargs, latent_factor_topologies):\n",
    "    \"\"\"Instantiates model, loss function, and optimizer based on config.\"\"\"\n",
    "    img_size = dataset[0][0].shape\n",
    "    n_data = len(dataset)\n",
    "    \n",
    "    # Instantiate S-N-VAE Model\n",
    "    model = vae_models.select(name=model_name, \n",
    "                              img_size=img_size, \n",
    "                              latent_factor_topologies=latent_factor_topologies,\n",
    "                              decoder_output_dist=model_decoder_output_dist\n",
    "                              ).to(device)\n",
    "\n",
    "    # Instantiate S-N-Annealed-VAE Loss\n",
    "    loss_fn = losses.select(loss_name, **loss_kwargs)\n",
    "\n",
    "    # Instantiate Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f\"--- Setup for {dataset.__class__.__name__} --- \")\n",
    "    print(f\"Model: {model.model_name}\")\n",
    "    print(f\"Latent factor topologies: {latent_factor_topologies}\")\n",
    "    print(f\"Loss: {loss_fn.name} (rec_dist={loss_kwargs['rec_dist']}), kwargs={loss_kwargs}\")\n",
    "    print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "\n",
    "    return model, loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018f307e",
   "metadata": {},
   "source": [
    "# dSprites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dSprites\n",
    "Dsprites = get_dataset('dsprites')\n",
    "\n",
    "# dsprites_dataset = Dsprites(selected_factors='all', not_selected_factors_index_value=None)\n",
    "dsprites_dataset = Dsprites(selected_factors=['posX', 'posY', 'orientation'], not_selected_factors_index_value={'scale':5, 'shape':0, 'color':0})\n",
    "\n",
    "# num_workers_dsprites = find_optimal_num_workers(dsprites_dataset, batch_size=batch_size, num_batches_to_test='all')\n",
    "num_workers_dsprites = 7\n",
    "\n",
    "if determinism_kwargs is not None:\n",
    "    dsprites_dataloader = get_deterministic_dataloader(dataset=dsprites_dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=num_workers_dsprites,\n",
    "                                                   seed=determinism_kwargs['seed'],\n",
    "                                                   pin_memory=True)\n",
    "else:\n",
    "    dsprites_dataloader = torch.utils.data.DataLoader(dsprites_dataset, \n",
    "                                                      batch_size=batch_size, \n",
    "                                                      num_workers=num_workers_dsprites, \n",
    "                                                      shuffle=True, \n",
    "                                                      pin_memory=True)\n",
    "\n",
    "print(f\"Loaded dSprites dataset with {len(dsprites_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b23ec6",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Training S-N-Annealed-VAE on dSprites =====\")\n",
    "model_dsprites, loss_fn_dsprites, optimizer_dsprites = setup_components(dsprites_dataset, \n",
    "                                                                        loss_kwargs_dsprites,\n",
    "                                                                        latent_factor_topologies\n",
    "                                                                        )\n",
    "trainer_dsprites = UnsupervisedTrainer(model=model_dsprites,\n",
    "                                      loss=loss_fn_dsprites,\n",
    "                                      optimizer=optimizer_dsprites,\n",
    "                                      lr_scheduler=None,\n",
    "                                      determinism_kwargs=determinism_kwargs,\n",
    "                                      use_torch_compile=use_torch_compile,\n",
    "                                      return_logs=True,\n",
    "                                      return_chkpt=return_chkpt,\n",
    "                                      chkpt_save_path=chkpt_save_path,\n",
    "                                      chkpt_save_dir=chkpt_save_dir,\n",
    "                                      chkpt_every_n_steps=chkpt_every_n_steps,\n",
    "                                      chkpt_viz=chkpt_viz\n",
    "                                      )\n",
    "\n",
    "trainer_dsprites.train(max_steps=num_train_steps, step_unit=train_step_unit, dataloader=dsprites_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310022ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Visualizing dSprites Results =====\")\n",
    "visualizer_dsprites = utils.visualize.SNVAEVisualizer(vae_model=model_dsprites, dataset=dsprites_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11632d31",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c99ff5",
   "metadata": {},
   "source": [
    "### Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e860e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting random reconstructions...\")\n",
    "visualizer_dsprites.plot_random_reconstructions(10, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting reconstructions from specific indices...\")\n",
    "indices_dsprites = [0, 10, 20, 30, 40, 50]  # Example indices\n",
    "visualizer_dsprites.plot_reconstructions_sub_dataset(indices_dsprites, mode='mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3accd02c",
   "metadata": {},
   "source": [
    "### Latent traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df957f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting latent traversals...\")\n",
    "visualizer_dsprites.plot_all_latent_traversals(num_samples=15,\n",
    "                                               r1_max_traversal_type='probability',\n",
    "                                               r1_max_traversal=0.95,\n",
    "                                               s1_max_traversal_type='fraction',\n",
    "                                               s1_max_traversal=1.0,\n",
    "                                               use_ref_img=True\n",
    "                                               )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a reference image index for dSprites\n",
    "ref_img_idx_dsprites = 495  # Example index\n",
    "ref_img_dsprites = dsprites_dataset[ref_img_idx_dsprites][0]\n",
    "\n",
    "# Plot the reference image\n",
    "plt.imshow(ref_img_dsprites.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "plt.title(f\"dSprites Reference Image (Index: {ref_img_idx_dsprites})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f414a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single latent traversal based on the reference image\n",
    "latent_factor_idx_dsprites = 0  # Index of the latent dimension to traverse\n",
    "print(f\"Plotting single latent traversal for dimension {latent_factor_idx_dsprites}...\")\n",
    "visualizer_dsprites.plot_single_latent_traversal(latent_factor_idx_dsprites, \n",
    "                                                 ref_img=ref_img_dsprites, \n",
    "                                                 num_samples=11,\n",
    "                                                 max_traversal_type='fraction',\n",
    "                                                 max_traversal=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5759983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All latent traversals based on the reference image\n",
    "print(\"Plotting all latent traversals based on reference image...\")\n",
    "visualizer_dsprites.plot_all_latent_traversals(ref_img=ref_img_dsprites, \n",
    "                                               num_samples=15,\n",
    "                                               r1_max_traversal_type='probability',\n",
    "                                               r1_max_traversal=0.95,\n",
    "                                               s1_max_traversal_type='fraction',\n",
    "                                               s1_max_traversal=1\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad3430",
   "metadata": {},
   "source": [
    "# G-Commutative VAE Extension\n",
    "\n",
    "In this section, we extend the S-N-Annealed-VAE model with Group Theory losses, specifically implementing g-commutative constraints. The g-commutative loss enforces commutativity properties in the latent space transformations, encouraging the model to learn more disentangled and structured representations.\n",
    "\n",
    "## Design Philosophy: Controlled Comparison\n",
    "**All basic training settings are kept identical** to the standard S-N-Annealed-VAE to ensure fair comparison. The **only difference** is the addition of group theory constraints on top of the base loss.\n",
    "\n",
    "## What Remains Identical:\n",
    "- ‚úÖ **Model Architecture**: Same S-N-VAE with Locatello encoder/decoder\n",
    "- ‚úÖ **Reconstruction Loss**: Same Bernoulli distribution and annealed capacity weighting  \n",
    "- ‚úÖ **Training Parameters**: Same learning rate, batch size, epochs, optimizer\n",
    "- ‚úÖ **Latent Topologies**: Same ['S1', 'R1', 'R1'] factor configuration\n",
    "- ‚úÖ **Base Loss Function**: Identical Annealed S-N-VAE loss as foundation\n",
    "\n",
    "## What Is Added (Group Theory Extensions):\n",
    "- üîÑ **Commutative Constraints**: Enforces g‚àòg' = g'‚àòg in latent space\n",
    "- üìà **Progressive Scheduling**: Gradually increases constraint strength during training  \n",
    "- ‚è∞ **Warm-up Period**: First 5000 steps use only base loss for stability\n",
    "- üéØ **Group Actions**: R¬π (translation) and S¬π (rotation) transformations\n",
    "\n",
    "This design ensures that any performance differences can be directly attributed to the group theory constraints rather than changes in basic training setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab4555",
   "metadata": {},
   "source": [
    "# G-Commutative Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb4fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### G-Commutative Loss Configuration ####\n",
    "# DESIGN PHILOSOPHY: Keep ALL basic training settings identical to standard S-N-Annealed-VAE\n",
    "# The ONLY difference is the addition of group theory constraints on top of the base loss\n",
    "\n",
    "# FIXED: Use correct loss name for S-N-VAE group theory\n",
    "g_commutative_loss_name = 'group_theory_snvae'  # S-N-VAE compatible group theory loss\n",
    "\n",
    "# Base loss configuration - use EXACT same settings as standard S-N-Annealed-VAE\n",
    "base_loss_g_commutative = {\n",
    "    'name': 'anneal_s_n_vae',\n",
    "    'kwargs': loss_kwargs_dsprites  # Identical to standard S-N-Annealed-VAE settings\n",
    "}\n",
    "\n",
    "# Group Theory Loss Parameters for G-Commutative\n",
    "loss_kwargs_g_commutative = {\n",
    "    'base_loss_name': base_loss_g_commutative['name'],\n",
    "    'base_loss_kwargs': base_loss_g_commutative['kwargs'],\n",
    "    'latent_factor_topologies': latent_factor_topologies,\n",
    "    'device': device,  # FIXED: Required parameter\n",
    "    \n",
    "    # KEEP IDENTICAL: Same reconstruction distribution as standard S-N-Annealed-VAE\n",
    "    'rec_dist': 'bernoulli',  # Same as standard S-N-Annealed-VAE for fair comparison\n",
    "    \n",
    "    ### ADDED GROUP THEORY CONSTRAINTS (only difference) ###\n",
    "    'commutative_weight': 1.0,                    # Enable commutative loss\n",
    "    'commutative_component_order': 2,             # Use pairs for commutative operations\n",
    "    'commutative_comparison_dist': 'gaussian',    # Comparison metric for commutative constraints\n",
    "    \n",
    "    ### Meaningful settings (disabled for pure g-commutative) ###\n",
    "    'meaningful_weight': 0.0,                     # Disable meaningful loss for pure commutative\n",
    "    'meaningful_component_order': 1,\n",
    "    'meaningful_transformation_order': 1,\n",
    "    'meaningful_critic_gradient_penalty_weight': 10.0,  # FIXED: Required parameter (even if not used)\n",
    "    'meaningful_critic_lr': 1e-4,                 # FIXED: Required parameter (even if not used)\n",
    "    'meaningful_n_critic': 1,\n",
    "    \n",
    "    ### Group theory general settings ###\n",
    "    'deterministic_rep': True,                    # Use deterministic representations  \n",
    "    'g_action_r1_range': 2.0,                    # Range for R1 (translation) actions\n",
    "    'g_action_s1_range': 2 * torch.pi,           # Full rotation range for S1 actions\n",
    "    'g_action_r1_dist': 'uniform',               # Distribution for R1 action sampling\n",
    "    'g_action_s1_dist': 'uniform',               # Distribution for S1 action sampling\n",
    "    'comp_latent_select_threshold': 0.1,         # Threshold for selecting latent components\n",
    "    'warm_up_steps': 5000,                       # Warm-up steps before applying group losses\n",
    "    \n",
    "    ### Learning rate scheduling ###\n",
    "    'schedulers_kwargs': [\n",
    "        {\n",
    "            'name': 'linear',\n",
    "            'kwargs': {\n",
    "                'param_name': 'commutative_weight',\n",
    "                'initial_value': 0.05,           # Start with low commutative weight\n",
    "                'final_value': 10.0,             # Gradually increase to strong constraint\n",
    "                'before_start_value': 0,\n",
    "                'start_step': 5000,              # Start scheduling after warm-up\n",
    "                'total_steps': int(num_train_steps * 0.8 if train_step_unit == 'epoch' \n",
    "                                 else num_train_steps * 0.8)  # Schedule over 80% of training\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=== TRAINING SETTINGS COMPARISON ===\")\n",
    "print(\"Standard S-N-Annealed-VAE vs G-Commutative S-N-Annealed-VAE:\")\n",
    "print(f\"‚úì Model architecture: IDENTICAL ({model_name})\")\n",
    "print(f\"‚úì Decoder output dist: IDENTICAL ({model_decoder_output_dist})\")\n",
    "print(f\"‚úì Latent topologies: IDENTICAL ({latent_factor_topologies})\")\n",
    "print(f\"‚úì Learning rate: IDENTICAL ({learning_rate})\")\n",
    "print(f\"‚úì Batch size: IDENTICAL ({batch_size})\")\n",
    "print(f\"‚úì Training steps: IDENTICAL ({num_train_steps} {train_step_unit}s)\")\n",
    "print(f\"‚úì Base loss: IDENTICAL ({base_loss_g_commutative['name']} with bernoulli)\")\n",
    "print(f\"‚úì Base loss params: IDENTICAL (gamma={loss_kwargs_dsprites['gamma']}, C_fin={loss_kwargs_dsprites['C_fin']})\")\n",
    "print(f\"+ ADDITIONAL: Group theory commutative constraints (weight={loss_kwargs_g_commutative['commutative_weight']})\")\n",
    "print(f\"+ ADDITIONAL: Warm-up period ({loss_kwargs_g_commutative['warm_up_steps']} steps)\")\n",
    "print(f\"üîß FIXED: Using correct loss name '{g_commutative_loss_name}' for S-N-VAE\")\n",
    "print(\"\\n‚Üí This ensures fair comparison: same base training + group constraints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ae9fb",
   "metadata": {},
   "source": [
    "## Setup Model, Loss, and Optimizer for G-Commutative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ea4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_g_commutative_components(dataset, loss_kwargs, latent_factor_topologies):\n",
    "    \"\"\"Instantiates model, group theory loss function, and optimizer for g-commutative training.\"\"\"\n",
    "    img_size = dataset[0][0].shape\n",
    "    n_data = len(dataset)\n",
    "    \n",
    "    # Instantiate S-N-VAE Model (same as before - the model architecture doesn't change)\n",
    "    model = vae_models.select(name=model_name, \n",
    "                              img_size=img_size, \n",
    "                              latent_factor_topologies=latent_factor_topologies,\n",
    "                              decoder_output_dist=model_decoder_output_dist\n",
    "                              ).to(device)\n",
    "\n",
    "    # Instantiate Group Theory Loss with G-Commutative constraints\n",
    "    loss_fn = losses.select(g_commutative_loss_name, **loss_kwargs)\n",
    "\n",
    "    # Instantiate Optimizer (same as before)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f\"--- G-Commutative Setup for {dataset.__class__.__name__} --- \")\n",
    "    print(f\"Model: {model.model_name}\")\n",
    "    print(f\"Latent factor topologies: {latent_factor_topologies}\")\n",
    "    print(f\"Loss: {loss_fn.name}\")\n",
    "    print(f\"  ‚Ü≥ Base loss: {loss_kwargs['base_loss_name']}\")\n",
    "    print(f\"  ‚Ü≥ Commutative weight: {loss_kwargs['commutative_weight']}\")\n",
    "    print(f\"  ‚Ü≥ Meaningful weight: {loss_kwargs['meaningful_weight']}\")\n",
    "    print(f\"  ‚Ü≥ Reconstruction dist: {loss_kwargs['rec_dist']}\")\n",
    "    print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "\n",
    "    return model, loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19238642",
   "metadata": {},
   "source": [
    "## Train G-Commutative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Training G-Commutative S-N-Annealed-VAE on dSprites =====\")\n",
    "model_g_comm_dsprites, loss_fn_g_comm_dsprites, optimizer_g_comm_dsprites = setup_g_commutative_components(\n",
    "    dsprites_dataset, \n",
    "    loss_kwargs_g_commutative,\n",
    "    latent_factor_topologies\n",
    ")\n",
    "\n",
    "# Setup trainer for G-Commutative model\n",
    "trainer_g_comm_dsprites = UnsupervisedTrainer(model=model_g_comm_dsprites,\n",
    "                                             loss=loss_fn_g_comm_dsprites,\n",
    "                                             optimizer=optimizer_g_comm_dsprites,\n",
    "                                             lr_scheduler=None,\n",
    "                                             determinism_kwargs=determinism_kwargs,\n",
    "                                             use_torch_compile=use_torch_compile,\n",
    "                                             return_logs=True,\n",
    "                                             return_chkpt=return_chkpt,\n",
    "                                             chkpt_save_path=chkpt_save_path,\n",
    "                                             chkpt_save_dir=chkpt_save_dir,\n",
    "                                             chkpt_every_n_steps=chkpt_every_n_steps,\n",
    "                                             chkpt_viz=chkpt_viz\n",
    "                                             )\n",
    "\n",
    "# Train the G-Commutative model\n",
    "print(f\"\\nStarting G-Commutative training for {num_train_steps} {train_step_unit}s...\")\n",
    "print(f\"Note: Group losses will be applied after {loss_kwargs_g_commutative['warm_up_steps']} warm-up steps\")\n",
    "trainer_g_comm_dsprites.train(max_steps=num_train_steps, step_unit=train_step_unit, dataloader=dsprites_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Visualizing G-Commutative dSprites Results =====\")\n",
    "visualizer_g_comm_dsprites = utils.visualize.SNVAEVisualizer(vae_model=model_g_comm_dsprites, dataset=dsprites_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b73bf",
   "metadata": {},
   "source": [
    "## G-Commutative Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da532f5",
   "metadata": {},
   "source": [
    "### G-Commutative Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48853e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting G-Commutative random reconstructions...\")\n",
    "visualizer_g_comm_dsprites.plot_random_reconstructions(10, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting G-Commutative reconstructions from specific indices...\")\n",
    "indices_g_comm_dsprites = [0, 10, 20, 30, 40, 50]  # Same indices for comparison\n",
    "visualizer_g_comm_dsprites.plot_reconstructions_sub_dataset(indices_g_comm_dsprites, mode='mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd8346",
   "metadata": {},
   "source": [
    "### G-Commutative Latent Traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting G-Commutative latent traversals...\")\n",
    "visualizer_g_comm_dsprites.plot_all_latent_traversals(num_samples=15,\n",
    "                                                      r1_max_traversal_type='probability',\n",
    "                                                      r1_max_traversal=0.95,\n",
    "                                                      s1_max_traversal_type='fraction',\n",
    "                                                      s1_max_traversal=1.0,\n",
    "                                                      use_ref_img=True\n",
    "                                                      )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aeb661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same reference image for direct comparison with standard S-N-Annealed-VAE results\n",
    "print(f\"Using the same reference image (Index: {ref_img_idx_dsprites}) for G-Commutative traversals...\")\n",
    "\n",
    "# Plot the reference image again for context\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(ref_img_dsprites.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "plt.title(f\"G-Commutative Reference Image (Index: {ref_img_idx_dsprites})\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Single latent traversal with G-Commutative model\n",
    "latent_factor_idx_g_comm = 0  # Same dimension as before\n",
    "print(f\"Plotting G-Commutative single latent traversal for dimension {latent_factor_idx_g_comm}...\")\n",
    "visualizer_g_comm_dsprites.plot_single_latent_traversal(latent_factor_idx_g_comm, \n",
    "                                                        ref_img=ref_img_dsprites, \n",
    "                                                        num_samples=11,\n",
    "                                                        max_traversal_type='fraction',\n",
    "                                                        max_traversal=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All latent traversals with G-Commutative model using the reference image\n",
    "print(\"Plotting all G-Commutative latent traversals based on reference image...\")\n",
    "visualizer_g_comm_dsprites.plot_all_latent_traversals(ref_img=ref_img_dsprites, \n",
    "                                                      num_samples=15,\n",
    "                                                      r1_max_traversal_type='probability',\n",
    "                                                      r1_max_traversal=0.95,\n",
    "                                                      s1_max_traversal_type='fraction',\n",
    "                                                      s1_max_traversal=1\n",
    "                                                      )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
