{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f696f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added project root: /notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root by looking for .git or requirements.txt\n",
    "current = Path.cwd()\n",
    "while not any((current / marker).exists() for marker in ['.git', 'requirements.txt']):\n",
    "    if current.parent == current:\n",
    "        raise FileNotFoundError(\"Project root not found\")\n",
    "    current = current.parent\n",
    "\n",
    "sys.path.append(str(current))\n",
    "print(f\"Added project root: {current}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239f71e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Add project root to path for imports\n",
    "# if not '..' in sys.path:\n",
    "#     sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76355d8b",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f524375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mixing_matrix(mixing_matrix, title=\"Ground Truth Mixing Matrix\"):\n",
    "    \"\"\"Visualize the relationship between factors and latents.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(mixing_matrix, annot=True, fmt=\".2f\", cmap=\"viridis\",\n",
    "                xticklabels=[f\"z{i}\" for i in range(mixing_matrix.shape[1])],\n",
    "                yticklabels=[f\"f{i}\" for i in range(mixing_matrix.shape[0])])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Latent Dimensions\")\n",
    "    plt.ylabel(\"Ground Truth Factors\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_metric_results(results, metric_name=\"DCId\"):\n",
    "    \"\"\"Visualize metric results across different disentanglement levels.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Extract relevant metrics\n",
    "    x = [r['level'] for r in results]\n",
    "    metrics = {k: [r[k] for r in results] for k in results[0].keys() if k != 'level'}\n",
    "    \n",
    "    # Plot each metric\n",
    "    for name, values in metrics.items():\n",
    "        plt.plot(x, values, 'o-', label=name)\n",
    "        \n",
    "    plt.xlabel('Disentanglement Level of Generated Data')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title(f'{metric_name} Metrics vs True Disentanglement Level')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a62d44",
   "metadata": {},
   "source": [
    "# Artificial disentanglement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c40409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(factor_sizes, num_latents, disentanglement_level, noise_level):\n",
    "    \"\"\"\n",
    "    Generate synthetic data with controlled disentanglement properties.\n",
    "    All possible combinations of factor values will be generated.\n",
    "    \n",
    "    Parameters:\n",
    "    - factor_sizes: List where each element specifies the number of discrete values for that factor\n",
    "                   (length of list determines number of factors)\n",
    "    - num_latents: Number of latent dimensions\n",
    "    - disentanglement_level: How disentangled the representation should be (0-1)\n",
    "    - noise_level: Amount of noise to add\n",
    "    \n",
    "    Returns:\n",
    "    - gt_factors: Ground truth factors [num_samples, num_factors]\n",
    "    - latent_reps: Latent representations [num_samples, num_latents]\n",
    "    - mixing_matrix: Matrix showing how factors map to latents\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    \n",
    "    # Determine number of factors from the length of factor_sizes\n",
    "    num_factors = len(factor_sizes)\n",
    "    \n",
    "    # Calculate total number of samples based on all combinations\n",
    "    num_samples = np.prod(factor_sizes)\n",
    "    \n",
    "    # Generate evenly spaced values for each factor\n",
    "    factor_values = []\n",
    "    for i in range(num_factors):\n",
    "        # Generate values from 0 to 1 with factor_sizes[i] discrete values\n",
    "        factor_values.append(np.linspace(0, 1, factor_sizes[i]))\n",
    "    \n",
    "    # Generate all combinations of factor values\n",
    "    all_combinations = list(itertools.product(*factor_values))\n",
    "    \n",
    "    # Create ground truth factors from all combinations\n",
    "    gt_factors = np.array(all_combinations)\n",
    "    \n",
    "    # Create a mixing matrix that defines how factors map to latents\n",
    "    # For perfect disentanglement, this would be an identity matrix (with zeros for any extra dimensions)\n",
    "    mixing_matrix = np.zeros((num_factors, num_latents))\n",
    "    \n",
    "    # Set up the mixing based on disentanglement level\n",
    "    for i in range(num_factors):\n",
    "        # Primary dimension gets most of the weight based on disentanglement level\n",
    "        primary_dim = i % num_latents  # In case we have more factors than latents\n",
    "        mixing_matrix[i, primary_dim] = disentanglement_level\n",
    "        \n",
    "        # Distribute remaining weight across other dimensions\n",
    "        remaining = 1.0 - disentanglement_level\n",
    "        other_dims = [j for j in range(num_latents) if j != primary_dim]\n",
    "        if other_dims:  # Check if there are other dimensions\n",
    "            for j in other_dims:\n",
    "                mixing_matrix[i, j] = remaining / len(other_dims)\n",
    "    \n",
    "    # Generate latent representations based on the mixing matrix\n",
    "    latent_reps = np.dot(gt_factors, mixing_matrix)\n",
    "    \n",
    "    # Add some noise to make it more realistic\n",
    "    latent_reps += np.random.normal(0, noise_level, latent_reps.shape)\n",
    "    \n",
    "    return gt_factors, latent_reps, mixing_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e17e1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 600000\n",
      "gt factors shape: (600000, 6) latent reps shape: (600000, 10) mixing matrix shape: (6, 10)\n"
     ]
    }
   ],
   "source": [
    "factor_sizes = [10, 10, 10, 8, 5, 15]  # Number of discrete values for each factor\n",
    "num_latents = 10  # Number of latent dimensions\n",
    "disentanglement_level = 0.5  # Disentanglement level (0-1)\n",
    "noise_level = 0.02  # Noise level to add to the latent representations\n",
    "total_samples = np.prod(factor_sizes)  # Total number of samples based on all combinations\n",
    "\n",
    "print(\"Total samples:\", total_samples)\n",
    "\n",
    "# Generate synthetic disentangled data\n",
    "gt_factors, latent_reps, mixing_matrix = generate_synthetic_data(\n",
    "    factor_sizes, num_latents, disentanglement_level, noise_level\n",
    ")\n",
    "\n",
    "print(\"gt factors shape:\", gt_factors.shape, \"latent reps shape:\", latent_reps.shape, \"mixing matrix shape:\", mixing_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17561ac",
   "metadata": {},
   "source": [
    "# DCI_d\n",
    "\n",
    "Testing the Disentanglement, Completeness, and Informativeness (DCI) metric with synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff48798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.dci_d import DCId\n",
    "\n",
    "\n",
    "train_ratio_dci_d = 0.8\n",
    "\n",
    "dci_d_num_train = int(train_ratio_dci_d * total_samples) \n",
    "dci_d_num_test = total_samples - dci_d_num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c47af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m dci_metric \u001b[38;5;241m=\u001b[39m DCId(num_train\u001b[38;5;241m=\u001b[39mdci_d_num_train, num_test\u001b[38;5;241m=\u001b[39mdci_d_num_test, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msklearn\u001b[39m\u001b[38;5;124m'\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Compute the metric\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m dci_results \u001b[38;5;241m=\u001b[39m \u001b[43mdci_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_reps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dci_results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/notebooks/metrics/dci_d.py:172\u001b[0m, in \u001b[0;36mDCId.__call__\u001b[0;34m(self, latent_reps, gt_factors, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m     res \u001b[38;5;241m=\u001b[39m [get_importance(factor_id) \u001b[38;5;28;01mfor\u001b[39;00m factor_id \u001b[38;5;129;01min\u001b[39;00m trange(num_factors, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_importance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactor_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfactor_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_factors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m importance_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([num_latents, num_factors])\n\u001b[1;32m    175\u001b[0m informativeness_train_scores \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert numpy arrays to torch tensors\n",
    "\n",
    "# Initialize the DCId metric\n",
    "dci_metric = DCId(num_train=dci_d_num_train, num_test=dci_d_num_test, backend='sklearn', num_workers=4)\n",
    "\n",
    "# Compute the metric\n",
    "dci_results = dci_metric(latent_reps, gt_factors)\n",
    "\n",
    "for key, value in dci_results.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d727d",
   "metadata": {},
   "source": [
    "# MIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7af63fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG Score: 0.5271\n"
     ]
    }
   ],
   "source": [
    "from metrics.mig import MIG\n",
    "\n",
    "# Set the parameters for MIG calculation\n",
    "mig_num_bins = 20\n",
    "mig_num_workers = 8\n",
    "\n",
    "# Initialize the MIG metric\n",
    "mig_metric = MIG(num_bins=mig_num_bins, num_workers=mig_num_workers, mi_method='pyitlib', entropy_method='pyitlib')\n",
    "\n",
    "# We can reuse the same tensors we used for DCI_d\n",
    "# Compute the metric\n",
    "mig_result = mig_metric(latent_reps, gt_factors)\n",
    "\n",
    "print(f\"MIG Score: {mig_result:.4f}\") # TODO check the correctness of this metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c14b2",
   "metadata": {},
   "source": [
    "# Modularity_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfcae0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseMetric.__init__() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m modularity_d_num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize the Modularityd metric\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m modularity_d_metric \u001b[38;5;241m=\u001b[39m \u001b[43mModularityd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodularity_d_num_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodularity_d_num_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmi_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumpy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Compute the metric\u001b[39;00m\n\u001b[1;32m     11\u001b[0m modularity_d_result \u001b[38;5;241m=\u001b[39m modularity_d_metric(latent_reps, gt_factors)\n",
      "File \u001b[0;32m/notebooks/metrics/modularity_d.py:35\u001b[0m, in \u001b[0;36mModularityd.__init__\u001b[0;34m(self, num_bins, num_workers, mi_method, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, mi_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msklearn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_bins \u001b[38;5;241m=\u001b[39m num_bins\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers \u001b[38;5;241m=\u001b[39m num_workers\n",
      "\u001b[0;31mTypeError\u001b[0m: BaseMetric.__init__() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "from metrics.modularity_d import Modularityd\n",
    "\n",
    "# Set the parameters for Modularityd calculation\n",
    "modularity_d_num_bins = 20\n",
    "modularity_d_num_workers = 4\n",
    "\n",
    "# Initialize the Modularityd metric\n",
    "modularity_d_metric = Modularityd(num_bins=modularity_d_num_bins, num_workers=modularity_d_num_workers, mi_method='numpy', device='cpu')\n",
    "\n",
    "# Compute the metric\n",
    "modularity_d_result = modularity_d_metric(latent_reps, gt_factors)\n",
    "print(f\"Modularityd Score: {modularity_d_result:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
