{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b422e02",
   "metadata": {},
   "source": [
    "# Î²-TCVAE Model Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4804603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.reproducibility import set_deterministic_run, get_deterministic_dataloader\n",
    "\n",
    "seed = 42\n",
    "set_deterministic_run(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e4ffc",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d16fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils.visualize\n",
    "from trainers import UnsupervisedTrainer\n",
    "import losses\n",
    "import vae_models\n",
    "from datasets import get_dataset\n",
    "from utils.io import find_optimal_num_workers\n",
    "from metrics.utils import MetricAggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e6ed7",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5997b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- General Hyperparameters ---\n",
    "model_name = 'vae_locatello'  # Name of the model architecture file (e.g., 'vae_burgess')\n",
    "latent_dim = 10\n",
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "rec_dist = 'bernoulli'  # Reconstruction distribution (e.g., 'bernoulli', 'gaussian')\n",
    "\n",
    "\n",
    "train_step_unit = 'epoch'  # Unit for training steps ('epoch' or 'iteration')\n",
    "num_train_steps = 5\n",
    "\n",
    "# train_step_unit = 'iteration'  # Unit for training steps ('epoch' or 'iteration')\n",
    "# num_train_steps = int(9e3)  # Number of training steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0cde49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loss Specific Hyperparameters ---\n",
    "# Beta-TCVAE\n",
    "loss_name = 'betatcvae'      # Type of loss ('betavae', 'annealedvae', 'betatcvae')\n",
    "loss_kwargs = {\n",
    "    'alpha': 1.0,    # Weight for mutual information term\n",
    "    'beta': 6.0,     # Weight for total correlation term\n",
    "    'gamma': 1.0,    # Weight for dimension-wise KL term\n",
    "    'is_mss': True,  # Use minibatch stratified sampling\n",
    "    'rec_dist': rec_dist,\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a56e35",
   "metadata": {},
   "source": [
    "## 3. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d8b42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3D Shapes dataset with 480000 samples.\n"
     ]
    }
   ],
   "source": [
    "# Load 3D Shapes\n",
    "Shapes3D = get_dataset(\"shapes3d\")\n",
    "shapes3d_dataset = Shapes3D(selected_factors='all', not_selected_factors_index_value=None)\n",
    "# num_workers_3dshapes = find_optimal_num_workers(shapes3d_dataset, batch_size=batch_size, num_batches_to_test='all')\n",
    "num_workers_3dshapes = 4\n",
    "\n",
    "# shapes3d_dataloader = torch.utils.data.DataLoader(shapes3d_dataset, batch_size=batch_size, num_workers=num_workers_3dshapes, shuffle=True, pin_memory=True)\n",
    "shapes3d_dataloader = get_deterministic_dataloader(dataset=shapes3d_dataset, \n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=num_workers_3dshapes,\n",
    "                                                   seed=seed,\n",
    "                                                   pin_memory=True)\n",
    "\n",
    "print(f\"Loaded 3D Shapes dataset with {len(shapes3d_dataset)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d7d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dSprites dataset with 737280 samples.\n"
     ]
    }
   ],
   "source": [
    "# Load dSprites\n",
    "Dsprites = get_dataset('dsprites')\n",
    "\n",
    "dsprites_dataset = Dsprites(selected_factors='all', not_selected_factors_index_value=None)\n",
    "# num_workers_dsprites = find_optimal_num_workers(dsprites_dataset, batch_size=batch_size, num_batches_to_test='all')\n",
    "num_workers_dsprites = 7\n",
    "\n",
    "# dsprites_dataloader = torch.utils.data.DataLoader(dsprites_dataset, batch_size=batch_size, num_workers=num_workers_dsprites, shuffle=True, pin_memory=True)\n",
    "dsprites_dataloader = get_deterministic_dataloader(dataset=dsprites_dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=num_workers_dsprites,\n",
    "                                                   seed=seed,\n",
    "                                                   pin_memory=True)\n",
    "\n",
    "print(f\"Loaded dSprites dataset with {len(dsprites_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042d8ee",
   "metadata": {},
   "source": [
    "## 4. Setup Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9eddea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_components(dataset, loss_kwargs):\n",
    "    \"\"\"Instantiates model, loss function, and optimizer based on config.\"\"\"\n",
    "    img_size = dataset[0][0].shape\n",
    "    n_data = len(dataset)\n",
    "    \n",
    "\n",
    "    # Instantiate Model\n",
    "    model = vae_models.select(name=model_name, img_size=img_size, latent_dim=latent_dim)\n",
    "\n",
    "    if loss_name == 'betatcvae':\n",
    "        loss_kwargs['n_data'] = n_data\n",
    "    \n",
    "    loss_fn = losses.select(loss_name, **loss_kwargs)\n",
    "\n",
    "    # Instantiate Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f\"--- Setup for {dataset.__class__.__name__} --- \")\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Loss: {loss_fn.__class__.__name__} (rec_dist={rec_dist}), kwargs={loss_kwargs}\")\n",
    "    print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "    print(f\"---------------------------\")\n",
    "\n",
    "    return model, loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc4014",
   "metadata": {},
   "source": [
    "## 5. Train and Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e2e61",
   "metadata": {},
   "source": [
    "## 5.1 - 3D Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8cd4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes3d_loss_kwargs = {\n",
    "    'alpha': 1.0,    # Weight for mutual information term\n",
    "    'beta': 6.0,     # Weight for total correlation term\n",
    "    'gamma': 1.0,    # Weight for dimension-wise KL term\n",
    "    'is_mss': True,  # Use minibatch stratified sampling\n",
    "    'rec_dist': rec_dist,\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fce39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training on 3D Shapes =====\n",
      "--- Setup for Shapes3D --- \n",
      "Model: Model\n",
      "Loss: Loss (rec_dist=bernoulli), kwargs={'alpha': 1.0, 'beta': 6.0, 'gamma': 1.0, 'is_mss': True, 'rec_dist': 'bernoulli', 'n_data': 480000}\n",
      "Optimizer: Adam\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'utils.math' has no attribute 'log_density_gaussian'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m model_3dshapes, loss_fn_3dshapes, optimizer_3dshapes \u001b[38;5;241m=\u001b[39m setup_components(shapes3d_dataset, shapes3d_loss_kwargs)\n\u001b[1;32m      4\u001b[0m trainer_3dshapes \u001b[38;5;241m=\u001b[39m UnsupervisedTrainer(model\u001b[38;5;241m=\u001b[39mmodel_3dshapes,\n\u001b[1;32m      5\u001b[0m                                       loss_fn\u001b[38;5;241m=\u001b[39mloss_fn_3dshapes,\n\u001b[1;32m      6\u001b[0m                                       lr_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                       train_step_unit\u001b[38;5;241m=\u001b[39mtrain_step_unit,\n\u001b[1;32m     10\u001b[0m                                       )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer_3dshapes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshapes3d_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_train_steps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/trainers/basetrainer.py:114\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self, data_loader, max_steps)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# _train_epoch returns dict if log_loss_interval_type=='epoch', list if 'iteration'\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m epoch_logs_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_log_loss:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_loss_interval_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;66;03m# epoch_logs_out already includes 'epoch' key from _train_epoch\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/trainers/basetrainer.py:221\u001b[0m, in \u001b[0;36mBaseTrainer._train_epoch\u001b[0;34m(self, data_loader, epoch)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data_out \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m    220\u001b[0m     data \u001b[38;5;241m=\u001b[39m data_out[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 221\u001b[0m     iter_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# Accumulate logs for overall epoch / progress bar\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, item \u001b[38;5;129;01min\u001b[39;00m iter_out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto_log\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/notebooks/trainers/basetrainer.py:280\u001b[0m, in \u001b[0;36mBaseTrainer._train_iteration\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    273\u001b[0m model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(samples)\n\u001b[1;32m    274\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: samples,\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_train\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_out,\n\u001b[1;32m    278\u001b[0m }\n\u001b[0;32m--> 280\u001b[0m loss_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/notebooks/losses/n_vae/betatcvae.py:75\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, data, reconstructions, stats_qzx, is_train, samples_qzx, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m rec_loss \u001b[38;5;241m=\u001b[39m reconstruction_loss(data, reconstructions, distribution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_dist)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m##### beta-TCVAE breaks down the standard KL-term in beta-VAE, KL[q(z|x_n)||p(z)], into multiple components including \u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m##### the total correlation, which the authors believe to be the most important aspect, and such which to scale independently:\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m### Compute Total Correlation (Part 2 in Eq. 2).\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Compute log(q((z_j)|x_i)) for every sample in batch: [batch_size x batch_size x latent_dim]\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# i.e. compute the probability of latents z_j under q(*|x_i) induced by sample x_i.\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m log_qzx_cross \u001b[38;5;241m=\u001b[39m \u001b[43mmath_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_density_gaussian\u001b[49m(\n\u001b[1;32m     76\u001b[0m     samples_qzx\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m*\u001b[39m[x\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m stats_qzx])\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# (Optional) Apply minibatch stratified sampling (c.f. Eq. S6) to log_qzx_cross[i, j, :].\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# In essence, we estimate q(z) using a minibatch {x_1, ..., x_m} for a z that was originally sampled from q(z|x_*).\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# See else Eq. S5 for a derivation.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mss:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'utils.math' has no attribute 'log_density_gaussian'"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== Training on 3D Shapes =====\")\n",
    "model_3dshapes, loss_fn_3dshapes, optimizer_3dshapes = setup_components(shapes3d_dataset, shapes3d_loss_kwargs)\n",
    "\n",
    "trainer_3dshapes = UnsupervisedTrainer(model=model_3dshapes,\n",
    "                                      loss_fn=loss_fn_3dshapes,\n",
    "                                      lr_scheduler=None,\n",
    "                                      optimizer=optimizer_3dshapes,\n",
    "                                      device=device,\n",
    "                                      train_step_unit=train_step_unit,\n",
    "                                      )\n",
    "\n",
    "trainer_3dshapes.train(shapes3d_dataloader, max_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba1a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Visualizing 3D Shapes Results =====\")\n",
    "visualizer_3dshapes = utils.visualize.Visualizer(vae_model=model_3dshapes, dataset=shapes3d_dataset)\n",
    "\n",
    "print(\"Plotting random reconstructions...\")\n",
    "visualizer_3dshapes.plot_random_reconstructions(10, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting reconstructions from specific indices...\")\n",
    "indices_3dshapes = [5000, 6000, 7000, 100, 1000, 1024] # Example indices\n",
    "visualizer_3dshapes.plot_reconstructions_sub_dataset(indices_3dshapes, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting latent traversals...\")\n",
    "visualizer_3dshapes.plot_all_latent_traversals(num_samples=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843a047",
   "metadata": {},
   "source": [
    "### 5.1.1 Metric Evaluation (3D Shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4bdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_compute = [\n",
    "    {'name': 'dci_d', 'args':{'num_train':5000, 'num_test':1000}}, # Added num_train and num_test\n",
    "    {'name': 'mig', 'args':{}} # MIG uses default args (num_bins=20, num_workers=8, etc.)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1689ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_aggregator_3dshapes = MetricAggregator(metrics=metrics_to_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Computing Metrics for 3D Shapes =====\")\n",
    "metrics_results_3dshapes = metric_aggregator_3dshapes.compute(model=model_3dshapes, \n",
    "                                                              data_loader=shapes3d_dataloader, \n",
    "                                                              device=device)\n",
    "print(\"3D Shapes Metrics:\", metrics_results_3dshapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0f690",
   "metadata": {},
   "source": [
    "## 5.2. Train and Visualize: dSprites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957191d2",
   "metadata": {},
   "source": [
    "### Loss args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb8ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta-TCVAE\n",
    "loss_kwargs_betatc_vae_dsprites = {\n",
    "    'alpha': 1.0,    # Weight for mutual information term\n",
    "    'beta': 8.0,     # Higher beta for dSprites dataset\n",
    "    'gamma': 1.0,    # Weight for dimension-wise KL term\n",
    "    'is_mss': True,  # Use minibatch stratified sampling\n",
    "    'rec_dist': rec_dist,\n",
    "    'log_kl_components': True\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e18e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Training on dSprites =====\")\n",
    "model_dsprites, loss_fn_dsprites, optimizer_dsprites = setup_components(dsprites_dataset, loss_kwargs_betatc_vae_dsprites)\n",
    "\n",
    "trainer_dsprites = UnsupervisedTrainer(model=model_dsprites,\n",
    "                                     loss_fn=loss_fn_dsprites,\n",
    "                                     lr_scheduler=None,\n",
    "                                     optimizer=optimizer_dsprites,\n",
    "                                     device=device,\n",
    "                                     train_step_unit=train_step_unit,\n",
    "                                     )\n",
    "\n",
    "trainer_dsprites.train(dsprites_dataloader, max_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Visualizing dSprites Results =====\")\n",
    "visualizer_dsprites = utils.visualize.Visualizer(vae_model=model_dsprites, dataset=dsprites_dataset)\n",
    "\n",
    "print(\"Plotting random reconstructions...\")\n",
    "visualizer_dsprites.plot_random_reconstructions(10, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting reconstructions from specific indices...\")\n",
    "indices_dsprites = [0, 100000, 200000, 300000, 40000, 50000] # Example indices\n",
    "visualizer_dsprites.plot_reconstructions_sub_dataset(indices_dsprites, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting latent traversals...\")\n",
    "visualizer_dsprites.plot_all_latent_traversals(num_samples=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7110ef3",
   "metadata": {},
   "source": [
    "### 5.2.1 Metric Evaluation (dSprites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_aggregator_dsprites = MetricAggregator(metrics=metrics_to_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Computing Metrics for dSprites =====\")\n",
    "metrics_results_dsprites = metric_aggregator_dsprites.compute(model=model_dsprites, \n",
    "                                                            data_loader=dsprites_dataloader, \n",
    "                                                            device=device)\n",
    "print(\"dSprites Metrics:\", metrics_results_dsprites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6167ea",
   "metadata": {},
   "source": [
    "## 6. Understanding Î²-TCVAE\n",
    "\n",
    "The Î²-TCVAE (Beta Total Correlation VAE) model decomposes the KL divergence term into three components:\n",
    "\n",
    "1. **Mutual Information** (scaled by `alpha`): Measures the amount of information between latent variables and data\n",
    "2. **Total Correlation** (scaled by `beta`): Measures the dependencies between latent dimensions \n",
    "3. **Dimension-wise KL** (scaled by `gamma`): Regularizes each latent dimension toward the prior\n",
    "\n",
    "The decomposition allows more targeted regularization of specific properties of the latent space, particularly focusing on the total correlation to encourage disentanglement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
