{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b422e02",
   "metadata": {},
   "source": [
    "# β-TCVAE Model Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4804603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.reproducibility import set_deterministic_run, get_deterministic_dataloader\n",
    "\n",
    "seed = 42\n",
    "set_deterministic_run(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e4ffc",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d16fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils.visualize\n",
    "from trainers import UnsupervisedTrainer\n",
    "import losses\n",
    "import vae_models\n",
    "from datasets import get_dataset\n",
    "from utils.io import find_optimal_num_workers\n",
    "from metrics.utils import MetricAggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e6ed7",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5997b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- General Hyperparameters ---\n",
    "model_name = 'vae_locatello'  # Name of the model architecture file (e.g., 'vae_burgess')\n",
    "latent_dim = 10\n",
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "rec_dist = 'bernoulli'  # Reconstruction distribution (e.g., 'bernoulli', 'gaussian')\n",
    "\n",
    "\n",
    "train_step_unit = 'epoch'  # Unit for training steps ('epoch' or 'iteration')\n",
    "num_train_steps = 5\n",
    "\n",
    "# train_step_unit = 'iteration'  # Unit for training steps ('epoch' or 'iteration')\n",
    "# num_train_steps = int(9e3)  # Number of training steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0cde49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loss Specific Hyperparameters ---\n",
    "# Beta-TCVAE\n",
    "loss_name = 'betatcvae'      # Type of loss ('betavae', 'annealedvae', 'betatcvae')\n",
    "loss_kwargs = {\n",
    "    'alpha': 1.0,    # Weight for mutual information term\n",
    "    'beta': 6.0,     # Weight for total correlation term\n",
    "    'gamma': 1.0,    # Weight for dimension-wise KL term\n",
    "    'is_mss': True,  # Use minibatch stratified sampling\n",
    "    'rec_dist': rec_dist,\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a56e35",
   "metadata": {},
   "source": [
    "## 3. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d8b42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3D Shapes dataset with 480000 samples.\n"
     ]
    }
   ],
   "source": [
    "# Load 3D Shapes\n",
    "Shapes3D = get_dataset(\"shapes3d\")\n",
    "shapes3d_dataset = Shapes3D(selected_factors='all', not_selected_factors_index_value=None)\n",
    "# num_workers_3dshapes = find_optimal_num_workers(shapes3d_dataset, batch_size=batch_size, num_batches_to_test='all')\n",
    "num_workers_3dshapes = 4\n",
    "\n",
    "# shapes3d_dataloader = torch.utils.data.DataLoader(shapes3d_dataset, batch_size=batch_size, num_workers=num_workers_3dshapes, shuffle=True, pin_memory=True)\n",
    "shapes3d_dataloader = get_deterministic_dataloader(dataset=shapes3d_dataset, \n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=num_workers_3dshapes,\n",
    "                                                   seed=seed,\n",
    "                                                   pin_memory=True)\n",
    "\n",
    "print(f\"Loaded 3D Shapes dataset with {len(shapes3d_dataset)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d7d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dSprites dataset with 737280 samples.\n"
     ]
    }
   ],
   "source": [
    "# Load dSprites\n",
    "Dsprites = get_dataset('dsprites')\n",
    "\n",
    "dsprites_dataset = Dsprites(selected_factors='all', not_selected_factors_index_value=None)\n",
    "# num_workers_dsprites = find_optimal_num_workers(dsprites_dataset, batch_size=batch_size, num_batches_to_test='all')\n",
    "num_workers_dsprites = 7\n",
    "\n",
    "# dsprites_dataloader = torch.utils.data.DataLoader(dsprites_dataset, batch_size=batch_size, num_workers=num_workers_dsprites, shuffle=True, pin_memory=True)\n",
    "dsprites_dataloader = get_deterministic_dataloader(dataset=dsprites_dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=num_workers_dsprites,\n",
    "                                                   seed=seed,\n",
    "                                                   pin_memory=True)\n",
    "\n",
    "print(f\"Loaded dSprites dataset with {len(dsprites_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042d8ee",
   "metadata": {},
   "source": [
    "## 4. Setup Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9eddea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_components(dataset, loss_kwargs):\n",
    "    \"\"\"Instantiates model, loss function, and optimizer based on config.\"\"\"\n",
    "    img_size = dataset[0][0].shape\n",
    "    n_data = len(dataset)\n",
    "    \n",
    "\n",
    "    # Instantiate Model\n",
    "    model = vae_models.select(name=model_name, img_size=img_size, latent_dim=latent_dim)\n",
    "\n",
    "    if loss_name == 'betatcvae':\n",
    "        loss_kwargs['n_data'] = n_data\n",
    "    \n",
    "    loss_fn = losses.select(loss_name, **loss_kwargs)\n",
    "\n",
    "    # Instantiate Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f\"--- Setup for {dataset.__class__.__name__} --- \")\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Loss: {loss_fn.__class__.__name__} (rec_dist={rec_dist}), kwargs={loss_kwargs}\")\n",
    "    print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "    print(f\"---------------------------\")\n",
    "\n",
    "    return model, loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc4014",
   "metadata": {},
   "source": [
    "## 5. Train and Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e2e61",
   "metadata": {},
   "source": [
    "## 5.1 - 3D Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8cd4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes3d_loss_kwargs = {\n",
    "    'alpha': 1.0,    # Weight for mutual information term\n",
    "    'beta': 6.0,     # Weight for total correlation term\n",
    "    'gamma': 1.0,    # Weight for dimension-wise KL term\n",
    "    'is_mss': True,  # Use minibatch stratified sampling\n",
    "    'rec_dist': rec_dist,\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fce39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training on 3D Shapes =====\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== Training on 3D Shapes =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model_3dshapes, loss_fn_3dshapes, optimizer_3dshapes \u001b[38;5;241m=\u001b[39m \u001b[43msetup_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshapes3d_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshapes3d_loss_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m trainer_3dshapes \u001b[38;5;241m=\u001b[39m UnsupervisedTrainer(model\u001b[38;5;241m=\u001b[39mmodel_3dshapes,\n\u001b[1;32m      5\u001b[0m                                       loss_fn\u001b[38;5;241m=\u001b[39mloss_fn_3dshapes,\n\u001b[1;32m      6\u001b[0m                                       lr_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                       train_step_unit\u001b[38;5;241m=\u001b[39mtrain_step_unit,\n\u001b[1;32m     10\u001b[0m                                       )\n\u001b[1;32m     12\u001b[0m trainer_3dshapes\u001b[38;5;241m.\u001b[39mtrain(shapes3d_dataloader, max_steps\u001b[38;5;241m=\u001b[39mnum_train_steps)\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36msetup_components\u001b[0;34m(dataset, loss_kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetatcvae\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     11\u001b[0m     loss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_data\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m n_data\n\u001b[0;32m---> 13\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m \u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Instantiate Optimizer\u001b[39;00m\n\u001b[1;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "File \u001b[0;32m/notebooks/losses/__init__.py:27\u001b[0m, in \u001b[0;36mselect\u001b[0;34m(name, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Loss(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetatcvae\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlosses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mn_vae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbetatcvae\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Loss\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Loss(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madagvae\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/notebooks/losses/n_vae/betatcvae.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkl_div\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kl_normal_loss\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_annealing\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math \u001b[38;5;28;01mas\u001b[39;00m math_utils\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLoss\u001b[39;00m(baseloss\u001b[38;5;241m.\u001b[39mBaseLoss):\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Compute the decomposed KL loss with either minibatch weighted sampling or\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    minibatch stratified sampling according to [1]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m       autoencoders.\" Advances in Neural Information Processing Systems. 2018.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== Training on 3D Shapes =====\")\n",
    "model_3dshapes, loss_fn_3dshapes, optimizer_3dshapes = setup_components(shapes3d_dataset, shapes3d_loss_kwargs)\n",
    "\n",
    "trainer_3dshapes = UnsupervisedTrainer(model=model_3dshapes,\n",
    "                                      loss_fn=loss_fn_3dshapes,\n",
    "                                      lr_scheduler=None,\n",
    "                                      optimizer=optimizer_3dshapes,\n",
    "                                      device=device,\n",
    "                                      train_step_unit=train_step_unit,\n",
    "                                      )\n",
    "\n",
    "trainer_3dshapes.train(shapes3d_dataloader, max_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba1a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Visualizing 3D Shapes Results =====\")\n",
    "visualizer_3dshapes = utils.visualize.Visualizer(vae_model=model_3dshapes, dataset=shapes3d_dataset)\n",
    "\n",
    "print(\"Plotting random reconstructions...\")\n",
    "visualizer_3dshapes.plot_random_reconstructions(10, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting reconstructions from specific indices...\")\n",
    "indices_3dshapes = [5000, 6000, 7000, 100, 1000, 1024] # Example indices\n",
    "visualizer_3dshapes.plot_reconstructions_sub_dataset(indices_3dshapes, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting latent traversals...\")\n",
    "visualizer_3dshapes.plot_all_latent_traversals(num_samples=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843a047",
   "metadata": {},
   "source": [
    "### 5.1.1 Metric Evaluation (3D Shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4bdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_compute = [\n",
    "    {'name': 'dci_d', 'args':{'num_train':5000, 'num_test':1000}}, # Added num_train and num_test\n",
    "    {'name': 'mig', 'args':{}} # MIG uses default args (num_bins=20, num_workers=8, etc.)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1689ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_aggregator_3dshapes = MetricAggregator(metrics=metrics_to_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Computing Metrics for 3D Shapes =====\")\n",
    "metrics_results_3dshapes = metric_aggregator_3dshapes.compute(model=model_3dshapes, \n",
    "                                                              data_loader=shapes3d_dataloader, \n",
    "                                                              device=device)\n",
    "print(\"3D Shapes Metrics:\", metrics_results_3dshapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0f690",
   "metadata": {},
   "source": [
    "## 5.2. Train and Visualize: dSprites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957191d2",
   "metadata": {},
   "source": [
    "### Loss args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb8ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta-TCVAE\n",
    "loss_kwargs_betatc_vae_dsprites = {\n",
    "    'alpha': 1.0,    # Weight for mutual information term\n",
    "    'beta': 8.0,     # Higher beta for dSprites dataset\n",
    "    'gamma': 1.0,    # Weight for dimension-wise KL term\n",
    "    'is_mss': True,  # Use minibatch stratified sampling\n",
    "    'rec_dist': rec_dist,\n",
    "    'log_kl_components': True\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e18e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Training on dSprites =====\")\n",
    "model_dsprites, loss_fn_dsprites, optimizer_dsprites = setup_components(dsprites_dataset, loss_kwargs_betatc_vae_dsprites)\n",
    "\n",
    "trainer_dsprites = UnsupervisedTrainer(model=model_dsprites,\n",
    "                                     loss_fn=loss_fn_dsprites,\n",
    "                                     lr_scheduler=None,\n",
    "                                     optimizer=optimizer_dsprites,\n",
    "                                     device=device,\n",
    "                                     train_step_unit=train_step_unit,\n",
    "                                     )\n",
    "\n",
    "trainer_dsprites.train(dsprites_dataloader, max_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Visualizing dSprites Results =====\")\n",
    "visualizer_dsprites = utils.visualize.Visualizer(vae_model=model_dsprites, dataset=dsprites_dataset)\n",
    "\n",
    "print(\"Plotting random reconstructions...\")\n",
    "visualizer_dsprites.plot_random_reconstructions(10, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting reconstructions from specific indices...\")\n",
    "indices_dsprites = [0, 100000, 200000, 300000, 40000, 50000] # Example indices\n",
    "visualizer_dsprites.plot_reconstructions_sub_dataset(indices_dsprites, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting latent traversals...\")\n",
    "visualizer_dsprites.plot_all_latent_traversals(num_samples=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7110ef3",
   "metadata": {},
   "source": [
    "### 5.2.1 Metric Evaluation (dSprites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_aggregator_dsprites = MetricAggregator(metrics=metrics_to_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Computing Metrics for dSprites =====\")\n",
    "metrics_results_dsprites = metric_aggregator_dsprites.compute(model=model_dsprites, \n",
    "                                                            data_loader=dsprites_dataloader, \n",
    "                                                            device=device)\n",
    "print(\"dSprites Metrics:\", metrics_results_dsprites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6167ea",
   "metadata": {},
   "source": [
    "## 6. Understanding β-TCVAE\n",
    "\n",
    "The β-TCVAE (Beta Total Correlation VAE) model decomposes the KL divergence term into three components:\n",
    "\n",
    "1. **Mutual Information** (scaled by `alpha`): Measures the amount of information between latent variables and data\n",
    "2. **Total Correlation** (scaled by `beta`): Measures the dependencies between latent dimensions \n",
    "3. **Dimension-wise KL** (scaled by `gamma`): Regularizes each latent dimension toward the prior\n",
    "\n",
    "The decomposition allows more targeted regularization of specific properties of the latent space, particularly focusing on the total correlation to encourage disentanglement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
