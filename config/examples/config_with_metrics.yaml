# Main configuration file for disentanglement learning training with metrics evaluation
# This file demonstrates training a Beta-VAE on the DSprites dataset with post-training metric evaluation

defaults:
  - base_config
  - dataset: dsprites
  - model: vae_burgess
  - loss: betavae
  - _self_

# Experiment settings (single seed for single training)
experiment_id: null  # Auto-generated
seeds: [42]  # Single seed for single training
results_dir: "experiments"
resume: true

# Trainer configuration
trainer:
  # Override default values
  step_unit: "epoch"
  max_steps: 50

  # Training settings
  determinism:
    use_cuda_det: true
    enforce_det: false

  # Model settings  
  model:
    name: "vae_burgess"
    latent_dim: 10

  # Loss settings
  loss:
    name: "betavae"
    beta: 4.0

  # Dataset settings
  dataset:
    name: "dsprites"
    root: "data/dsprites"
    train: true
    download: true

  # Dataloader settings
  dataloader:
    batch_size: 64
    num_workers: -1  # Auto-detect
    shuffle: true
    pin_memory: true

  # Progress tracking
  progress_bar:
    enabled: true
    log_iter_interval: 50

  # Logging
  logging:
    enabled: true
    return_logs: true
    loss_interval_type: "iter"
    loss_iter_interval: 200
    metrics_interval_type: "iter" 
    metrics_iter_interval: 200

  # Checkpointing
  checkpoint:
    enabled: true
    return_chkpt: false
    every_n_steps: 10
    step_type: "epoch"
    save_dir: "checkpoints/betavae_dsprites"
    save_viz: true

  # Performance optimization
  torch_compile:
    enabled: false
    mode: "max-autotune"
    backend: "inductor"

  # Metrics configuration for post-training evaluation
  metrics:
    name: "metric_aggregator"
    sample_num: 10000  # Number of samples to use for metric computation
    metrics:
      - name: "mig"
        num_bins: 20
        num_workers: 8
        mi_method: 'pyitlib'
        entropy_method: 'pyitlib'
      
      - name: "sap_d"
        num_train: 5000
        num_test: 2500
        num_bins: 20
      
      - name: "dci_d"
        num_train: 5000
        num_test: 2500
        split_ratio: 0.8
        backend: 'sklearn'
        num_workers: 8
      
      - name: "modularity_d"
        num_bins: 20
        num_workers: 8
        mi_method: 'sklearn'

  # Optimizer configuration
  optimizer:
    name: "adam"
    lr: 0.001
    weight_decay: 0.0

  # Learning rate scheduler
  lr_scheduler:
    name: null  # No scheduler
