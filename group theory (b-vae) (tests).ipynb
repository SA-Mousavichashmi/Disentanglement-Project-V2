{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2a216f",
   "metadata": {},
   "source": [
    "# Group Theory VAE (BetaVAE Base) Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ca1a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.reproducibility import set_deterministic_run, get_deterministic_dataloader\n",
    "\n",
    "seed = 42\n",
    "set_deterministic_run(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af18741c",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a4c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils\n",
    "from trainers import UnsupervisedTrainer\n",
    "import losses\n",
    "import vae_models\n",
    "from datasets import get_dataset\n",
    "from utils.io import find_optimal_num_workers\n",
    "from metrics.utils import MetricAggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed950e",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05087129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- General Hyperparameters ---\n",
    "model_name = 'vae_locatello'\n",
    "latent_dim = 10\n",
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "rec_dist = 'bernoulli'\n",
    "\n",
    "train_step_unit = 'epoch'\n",
    "num_train_steps = 5\n",
    "\n",
    "# train_step_unit = 'iteration'\n",
    "# num_train_steps = int(9e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "361efa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loss Specific Hyperparameters ---\n",
    "loss_name = 'group_theory'\n",
    "base_loss_name = 'betavae'\n",
    "\n",
    "group_loss_kwargs = {\n",
    "    'base_loss_name': base_loss_name,\n",
    "    'base_loss_kwargs': {\n",
    "        'rec_dist': rec_dist,\n",
    "        'latent_dim': latent_dim,\n",
    "        'beta': 16, # Default beta\n",
    "    },\n",
    "    'rec_dist': rec_dist,\n",
    "    'device': device,\n",
    "    'commutative_weight': 1.0,\n",
    "    'meaningful_weight': 1.0,\n",
    "    'commutative_component_order': 2,\n",
    "    'meaningful_component_order': 1,\n",
    "    'meaningful_transformation_order': 1,\n",
    "    'meaningful_critic_gradient_penalty_weight': 10,\n",
    "    'meaningful_critic_lr': 1e-4,\n",
    "    'meaningful_n_critic': 5,\n",
    "    'deterministic_rep': False,\n",
    "    'log_kl_components': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451df5f",
   "metadata": {},
   "source": [
    "## 3. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4523fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3D Shapes dataset with 480000 samples.\n"
     ]
    }
   ],
   "source": [
    "# Load 3D Shapes\n",
    "Shapes3D = get_dataset(\"shapes3d\")\n",
    "shapes3d_dataset = Shapes3D(selected_factors='all', not_selected_factors_index_value=None)\n",
    "num_workers_3dshapes = 4\n",
    "\n",
    "shapes3d_dataloader = get_deterministic_dataloader(dataset=shapes3d_dataset, \n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=num_workers_3dshapes,\n",
    "                                                   seed=seed,\n",
    "                                                   pin_memory=True)\n",
    "\n",
    "print(f\"Loaded 3D Shapes dataset with {len(shapes3d_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3017c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dSprites dataset with 737280 samples.\n"
     ]
    }
   ],
   "source": [
    "# Load dSprites\n",
    "Dsprites = get_dataset('dsprites')\n",
    "dsprites_dataset = Dsprites(selected_factors='all', not_selected_factors_index_value=None)\n",
    "num_workers_dsprites = 7\n",
    "\n",
    "dsprites_dataloader = get_deterministic_dataloader(dataset=dsprites_dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=num_workers_dsprites,\n",
    "                                                   seed=seed,\n",
    "                                                   pin_memory=True)\n",
    "\n",
    "print(f\"Loaded dSprites dataset with {len(dsprites_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889adcce",
   "metadata": {},
   "source": [
    "## 4. Setup Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc3a3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_components(dataset, specific_loss_kwargs):\n",
    "    \"\"\"Instantiates model, loss function, and optimizer based on config.\"\"\"\n",
    "    img_size = dataset[0][0].shape\n",
    "    n_data = len(dataset)\n",
    "    \n",
    "    model = vae_models.select(name=model_name, img_size=img_size, latent_dim=latent_dim)\n",
    "    \n",
    "    final_loss_kwargs = group_loss_kwargs.copy()\n",
    "    final_loss_kwargs.update(specific_loss_kwargs)\n",
    "    \n",
    "    if final_loss_kwargs.get('base_loss') == 'betatcvae':\n",
    "         final_loss_kwargs['n_data'] = n_data\n",
    "         \n",
    "    loss_fn = losses.select(loss_name, **final_loss_kwargs)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f\"--- Setup for {dataset.__class__.__name__} --- \")\n",
    "    print(f\"Model: {model.__class__.__name__}\" )\n",
    "    print(f\"Loss: {loss_fn.__class__.__name__} (Base: {base_loss_name}, rec_dist={rec_dist}), kwargs={final_loss_kwargs}\")\n",
    "    print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "    print(f\"---------------------------\")\n",
    "\n",
    "    return model, loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e1cbc",
   "metadata": {},
   "source": [
    "## 5. Train and Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e3015",
   "metadata": {},
   "source": [
    "### 5.1 - 3D Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2344444",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes3d_specific_loss_kwargs = group_loss_kwargs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8285c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training on 3D Shapes =====\n",
      "--- Setup for Shapes3D --- \n",
      "Model: Model\n",
      "Loss: Loss (Base: betavae, rec_dist=bernoulli), kwargs={'base_loss_name': 'betavae', 'base_loss_kwargs': {'rec_dist': 'bernoulli', 'latent_dim': 10, 'beta': 16}, 'rec_dist': 'bernoulli', 'device': device(type='cuda'), 'commutative_weight': 1.0, 'meaningful_weight': 1.0, 'commutative_component_order': 2, 'meaningful_component_order': 1, 'meaningful_transformation_order': 1, 'meaningful_critic_gradient_penalty_weight': 10, 'meaningful_critic_lr': 0.0001, 'meaningful_n_critic': 5, 'deterministic_rep': False, 'log_kl_components': True}\n",
      "Optimizer: Adam\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as src tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m      2\u001b[0m model_3dshapes, loss_fn_3dshapes, optimizer_3dshapes \u001b[38;5;241m=\u001b[39m setup_components(shapes3d_dataset, shapes3d_specific_loss_kwargs)\n\u001b[1;32m      4\u001b[0m trainer_3dshapes \u001b[38;5;241m=\u001b[39m UnsupervisedTrainer(model\u001b[38;5;241m=\u001b[39mmodel_3dshapes,\n\u001b[1;32m      5\u001b[0m                                loss_fn\u001b[38;5;241m=\u001b[39mloss_fn_3dshapes,\n\u001b[1;32m      6\u001b[0m                                optimizer\u001b[38;5;241m=\u001b[39moptimizer_3dshapes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m                                log_loss_interval_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m                                )\n\u001b[0;32m---> 14\u001b[0m train_logs_3dshapes \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_3dshapes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshapes3d_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_train_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Logs (3D Shapes):\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_logs_3dshapes)\n",
      "File \u001b[0;32m/notebooks/trainers/basetrainer.py:114\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self, data_loader, max_steps)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# _train_epoch returns dict if log_loss_interval_type=='epoch', list if 'iteration'\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m epoch_logs_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_log_loss:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_loss_interval_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;66;03m# epoch_logs_out already includes 'epoch' key from _train_epoch\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/trainers/basetrainer.py:221\u001b[0m, in \u001b[0;36mBaseTrainer._train_epoch\u001b[0;34m(self, data_loader, epoch)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data_out \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m    220\u001b[0m     data \u001b[38;5;241m=\u001b[39m data_out[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 221\u001b[0m     iter_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# Accumulate logs for overall epoch / progress bar\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, item \u001b[38;5;129;01min\u001b[39;00m iter_out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto_log\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/notebooks/trainers/basetrainer.py:299\u001b[0m, in \u001b[0;36mBaseTrainer._train_iteration\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizes_internally\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 299\u001b[0m     loss_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/notebooks/losses/group_theory/group_theory.py:235\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, data, model, vae_optimizer)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeaningful_n_critic):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# Generate 'fake_images' without gradient for the critic\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 235\u001b[0m         fake_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_multiple_group_actions_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreal_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkl_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkl_components_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass kl_components\u001b[39;49;00m\n\u001b[1;32m    239\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvariance_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariance_components\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass variance_components\u001b[39;49;00m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     critic_real \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(data)\n\u001b[1;32m    243\u001b[0m     critic_fake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(fake_images)\n",
      "File \u001b[0;32m/notebooks/losses/group_theory/group_theory.py:160\u001b[0m, in \u001b[0;36mLoss._apply_multiple_group_actions_images\u001b[0;34m(self, real_images, model, kl_components, variance_components)\u001b[0m\n\u001b[1;32m    158\u001b[0m z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_representations(fake_images, is_deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic_rep)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Generate random transformation parameters using imported function\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m transform_params \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_random_latent_translation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomponent_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeaningful_component_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkl_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkl_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass kl_components\u001b[39;49;00m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariance_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariance_components\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass variance_components\u001b[39;49;00m\n\u001b[1;32m    166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Apply group action in latent space using imported function\u001b[39;00m\n\u001b[1;32m    168\u001b[0m z_transformed \u001b[38;5;241m=\u001b[39m apply_group_action_latent_space(transform_params, z)\n",
      "File \u001b[0;32m/notebooks/losses/group_theory/utils.py:63\u001b[0m, in \u001b[0;36mgenerate_random_latent_translation\u001b[0;34m(batch_size, latent_dim, component_order, kl_components, variance_components)\u001b[0m\n\u001b[1;32m     58\u001b[0m transformation_values \u001b[38;5;241m=\u001b[39m random_samples \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(selected_variances) \u001b[38;5;66;03m# TODO, this must be checked for getting square root of variance\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Place the sampled and scaled values into the transformation_parameters tensor\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# scatter_(dim, index, src) -> self[index[i][j]][j] = src[i][j] for dim=0\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# scatter_(dim, index, src) -> self[i][index[i][j]] = src[i][j] for dim=1\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[43mtransformation_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformation_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformation_parameters\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as src tensor"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== Training on 3D Shapes =====\")\n",
    "model_3dshapes, loss_fn_3dshapes, optimizer_3dshapes = setup_components(shapes3d_dataset, shapes3d_specific_loss_kwargs)\n",
    "\n",
    "trainer_3dshapes = UnsupervisedTrainer(model=model_3dshapes,\n",
    "                               loss_fn=loss_fn_3dshapes,\n",
    "                               optimizer=optimizer_3dshapes,\n",
    "                               lr_scheduler=None,\n",
    "                               device=device,\n",
    "                               train_step_unit=train_step_unit,\n",
    "                               return_log_loss=True,\n",
    "                               log_loss_interval_type='epoch'\n",
    "                               )\n",
    "\n",
    "train_logs_3dshapes = trainer_3dshapes.train(shapes3d_dataloader, max_steps=num_train_steps)\n",
    "print(\"Training Logs (3D Shapes):\", train_logs_3dshapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675f0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Visualizing 3D Shapes Results =====\")\n",
    "visualizer_3dshapes = utils.visualize.Visualizer(vae_model=model_3dshapes, dataset=shapes3d_dataset)\n",
    "\n",
    "print(\"Plotting random reconstructions...\")\n",
    "visualizer_3dshapes.plot_random_reconstructions(10, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting reconstructions from specific indices...\")\n",
    "indices_3dshapes = [5000, 6000, 7000, 100, 1000, 1024]\n",
    "visualizer_3dshapes.plot_reconstructions_sub_dataset(indices_3dshapes, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting latent traversals...\")\n",
    "visualizer_3dshapes.plot_all_latent_traversals(num_samples=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd84d1",
   "metadata": {},
   "source": [
    "### 5.1.1 Metric Evaluation (3D Shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ecf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_compute = [\n",
    "    {'name': 'dci_d', 'args':{'num_train':5000, 'num_test':1000}},\n",
    "    {'name': 'mig', 'args':{}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ccc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_aggregator_3dshapes = MetricAggregator(metrics=metrics_to_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Computing Metrics for 3D Shapes =====\")\n",
    "metrics_results_3dshapes = metric_aggregator_3dshapes.compute(model=model_3dshapes, \n",
    "                                                              data_loader=shapes3d_dataloader, \n",
    "                                                              device=device)\n",
    "print(\"3D Shapes Metrics:\", metrics_results_3dshapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c545e4d",
   "metadata": {},
   "source": [
    "### 5.2 - dSprites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsprites_specific_loss_kwargs = {\n",
    "    # 'beta': 4 # Uses default from group_loss_kwargs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Training on dSprites =====\")\n",
    "model_dsprites, loss_fn_dsprites, optimizer_dsprites = setup_components(dsprites_dataset, dsprites_specific_loss_kwargs)\n",
    "\n",
    "trainer_dsprites = UnsupervisedTrainer(model=model_dsprites,\n",
    "                               loss_fn=loss_fn_dsprites,\n",
    "                               optimizer=optimizer_dsprites,\n",
    "                               lr_scheduler=None,\n",
    "                               device=device,\n",
    "                               train_step_unit=train_step_unit,\n",
    "                               return_log_loss=True,\n",
    "                               log_loss_interval_type='epoch'\n",
    "                               )\n",
    "\n",
    "train_logs_dsprites = trainer_dsprites.train(dsprites_dataloader, max_steps=num_train_steps)\n",
    "print(\"Training Logs (dSprites):\", train_logs_dsprites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Visualizing dSprites Results =====\")\n",
    "visualizer_dsprites = utils.visualize.Visualizer(vae_model=model_dsprites, dataset=dsprites_dataset)\n",
    "\n",
    "print(\"Plotting random reconstructions...\")\n",
    "visualizer_dsprites.plot_random_reconstructions(10, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting reconstructions from specific indices...\")\n",
    "indices_dsprites = [0, 100000, 200000, 300000, 40000, 50000]\n",
    "visualizer_dsprites.plot_reconstructions_sub_dataset(indices_dsprites, mode='mean')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting latent traversals...\")\n",
    "visualizer_dsprites.plot_all_latent_traversals(num_samples=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f715f35",
   "metadata": {},
   "source": [
    "### 5.2.1 Metric Evaluation (dSprites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ac6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_aggregator_dsprites = MetricAggregator(metrics=metrics_to_compute)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
